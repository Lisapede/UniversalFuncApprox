{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest words via embeddings\n",
    "Here is the prompt to generate this code in ChatGPT\n",
    "Write a python program using pytorch to create embeddings for a list of words. Then given the input of one of the words find the 5 closest words to it. \n",
    "\n",
    "Question - Are the embeddings normalized? ie are they of length 1 ?\n",
    "\n",
    "## Exercise in Class\n",
    "Add a capability to embed 5 phrases containing up to 30 words, \n",
    "then given a query select the phrase that might answer that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn' from '/home/codespace/.local/lib/python3.12/site-packages/torch/nn/__init__.py'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for the first two words:\n",
      "tensor([[-0.6265, -0.8561,  0.6741,  1.9465,  1.0323, -1.2304, -0.8675,  1.3524,\n",
      "          2.0767, -0.5330, -0.0208,  0.7816, -0.2869,  1.6492, -0.6981, -0.7236,\n",
      "          0.5189, -0.0886, -0.4306,  2.0238, -0.0951,  0.0615,  1.1272, -0.7233,\n",
      "          0.5621, -1.7471,  1.5854,  0.8586, -0.4335, -1.3795,  0.7495, -1.5988,\n",
      "         -0.7011, -0.2762, -0.0536, -0.9139, -0.2065,  0.8348,  0.2882,  0.5704,\n",
      "          1.2892,  0.3607, -0.1623,  0.2154, -0.0061, -0.5041,  0.7950,  0.7241,\n",
      "          0.3391, -0.1953],\n",
      "        [-2.4873, -0.8843,  1.1065,  0.9266,  1.1889, -0.7478, -0.7300,  1.2461,\n",
      "          0.5420,  1.2264, -0.8877,  0.6605, -1.0670,  1.8461,  0.0741, -0.8376,\n",
      "          0.0667, -1.8449,  0.1929,  0.6504,  1.2975, -0.9518, -0.2281,  1.3243,\n",
      "         -1.4686,  1.1725, -1.0446, -1.3329, -1.8933,  1.5292, -0.2723,  0.1359,\n",
      "         -1.7234, -1.0254, -0.3054, -1.7177, -1.4470,  0.4552,  0.2994, -0.2911,\n",
      "          0.1374,  1.8789, -0.2257, -0.4423,  0.1201,  0.2786,  0.1179,  0.2883,\n",
      "          2.1338,  1.2735]], grad_fn=<SliceBackward0>)\n",
      "Embeddings for the last two words:\n",
      "tensor([[ 0.9206, -0.3257, -0.9240,  1.0455,  0.4595, -2.3223,  0.3395, -1.1690,\n",
      "          0.5025, -0.6231,  0.9537,  0.6270,  0.3969, -0.7208, -0.0852, -0.8061,\n",
      "          0.4746,  0.0758, -0.1861, -0.3521, -0.4850, -0.3328, -0.4190, -0.8076,\n",
      "          0.5750, -0.3288,  0.1967, -1.1878, -1.5649,  0.5135, -1.4126, -1.6153,\n",
      "         -2.0396, -1.7228,  0.1074,  0.3721,  0.6252, -0.9021, -0.7418,  0.1519,\n",
      "         -0.3282, -0.7459,  0.0375, -0.8958, -0.7618, -0.3758, -0.5940, -0.5258,\n",
      "          0.0633, -0.4373],\n",
      "        [-1.8251, -0.4443, -0.1554, -0.3578,  1.4698,  0.8538, -0.0885,  0.0649,\n",
      "         -0.8925, -0.5937,  1.5810, -0.4841, -0.3678, -0.5309,  1.2063, -0.2642,\n",
      "          1.3653,  0.0460, -1.0396,  0.4267,  0.2359,  0.7174, -0.7000,  1.4368,\n",
      "          0.0773,  0.9835,  0.2287,  2.7862, -0.4819,  0.5512,  2.1349,  0.7028,\n",
      "         -0.5024,  1.0927, -1.3740, -0.5336,  0.9231,  0.3456, -0.9702, -0.2815,\n",
      "          0.1122,  0.3283,  0.2255,  0.0846,  1.8487, -0.3301, -0.1541, -0.2718,\n",
      "          1.2040,  0.1859]], grad_fn=<SliceBackward0>)\n",
      "Embedding for the word 'apple':\n",
      "tensor([-0.6265, -0.8561,  0.6741,  1.9465,  1.0323, -1.2304, -0.8675,  1.3524,\n",
      "         2.0767, -0.5330, -0.0208,  0.7816, -0.2869,  1.6492, -0.6981, -0.7236,\n",
      "         0.5189, -0.0886, -0.4306,  2.0238, -0.0951,  0.0615,  1.1272, -0.7233,\n",
      "         0.5621, -1.7471,  1.5854,  0.8586, -0.4335, -1.3795,  0.7495, -1.5988,\n",
      "        -0.7011, -0.2762, -0.0536, -0.9139, -0.2065,  0.8348,  0.2882,  0.5704,\n",
      "         1.2892,  0.3607, -0.1623,  0.2154, -0.0061, -0.5041,  0.7950,  0.7241,\n",
      "         0.3391, -0.1953], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a list of words (vocabulary)\n",
    "words = [\"apple\", \"banana\", \"orange\", \"pear\", \"peach\", \n",
    "         \"mango\", \"grape\", \"cherry\", \"berry\", \"melon\"]\n",
    "\n",
    "# Create mappings from word to index and index to word\n",
    "word2idx = {word: idx for idx, word in enumerate(words)}\n",
    "idx2word = {idx: word for idx, word in enumerate(words)}\n",
    "\n",
    "# Set embedding dimensions and create the embedding layer\n",
    "embedding_dim = 50\n",
    "embedding_layer = nn.Embedding(num_embeddings=len(words), embedding_dim=embedding_dim)\n",
    "\n",
    "# Get embeddings for all words in the vocabulary\n",
    "# (This will be a matrix of shape [vocab_size, embedding_dim])\n",
    "embeddings = embedding_layer(torch.arange(len(words)))\n",
    "# print the embedding for the first two words\n",
    "print(\"Embeddings for the first two words:\")\n",
    "print(embeddings[:2])\n",
    "# print the embedding for the last two words\n",
    "print(\"Embeddings for the last two words:\")\n",
    "print(embeddings[-2:])\n",
    "# print the embedding for the word \"apple\"\n",
    "print(\"Embedding for the word 'apple':\")\n",
    "print(embeddings[word2idx[\"apple\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(word, top_k=5):\n",
    "    \"\"\"Finds the top_k closest words to the input word using cosine similarity.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return []\n",
    "    \n",
    "    # Get the embedding for the input word\n",
    "    word_index = word2idx[word]\n",
    "    word_embedding = embeddings[word_index]\n",
    "    \n",
    "    # Normalize all embeddings to unit length\n",
    "    normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    normalized_word_embedding = F.normalize(word_embedding, p=2, dim=0)\n",
    "    \n",
    "    # Compute cosine similarities: dot product between normalized vectors\n",
    "    similarities = torch.matmul(normalized_embeddings, normalized_word_embedding)\n",
    "    \n",
    "    # Get the indices of the top (top_k+1) similar words (including the word itself)\n",
    "    top_values, top_indices = torch.topk(similarities, top_k + 1)\n",
    "    \n",
    "    results = []\n",
    "    for value, idx in zip(top_values, top_indices):\n",
    "        # Skip the word itself\n",
    "        if idx.item() == word_index:\n",
    "            continue\n",
    "        results.append((idx2word[idx.item()], value.item()))\n",
    "        if len(results) == top_k:\n",
    "            break\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 'banana':\n",
      "grape (cosine similarity: 0.1776)\n",
      "orange (cosine similarity: 0.0944)\n",
      "cherry (cosine similarity: 0.0777)\n",
      "berry (cosine similarity: 0.0662)\n",
      "peach (cosine similarity: 0.0537)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_word = \"banana\"\n",
    "closest_words = find_closest(input_word)\n",
    "\n",
    "if closest_words:\n",
    "    print(f\"Top {len(closest_words)} words similar to '{input_word}':\")\n",
    "    for word, similarity in closest_words:\n",
    "        print(f\"{word} (cosine similarity: {similarity:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
